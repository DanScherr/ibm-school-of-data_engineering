{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercicios**\n",
    "\n",
    "1. encontrar inconsistencias entre os campos tx_tip_pgto_tran, nm_lncd, tx_idfr_pcl, qt_pcl_tran\n",
    "2. tratar o campo tx_idfr_pcl (deve ser o numero da parcela que está sendo paga), removendo strings e trazendo apenas integers;\n",
    "3. tratar o campo vl_moen (quando tx_tip_lcto for DEBITO, o vl_moen deve ser negativo, e quando for CREDITO, deve ser positivo) [observar valores já negativos - tirar o módulo da coluna antes de transformar]];\n",
    "4. tratar o campo nm_tran (ifood e 99app - padronizar a escrita)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Initial Configs**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\017924631\\desktop\\working-tree\\ibm-school-of-data_engineering\\6-spark\\ambvir\\lib\\site-packages (22.3.1)\n",
      "Collecting pip\n",
      "  Using cached pip-23.0-py3-none-any.whl (2.1 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.3.1\n",
      "    Uninstalling pip-22.3.1:\n",
      "      Successfully uninstalled pip-22.3.1\n",
      "Successfully installed pip-23.0\n",
      "Collecting pyspark\n",
      "  Using cached pyspark-3.3.1.tar.gz (281.4 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting py4j==0.10.9.5\n",
      "  Using cached py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py): started\n",
      "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845533 sha256=cf5911cf99fc008fc04a46b599c897eff24ec31ea0625e05327c383959fcf69a\n",
      "  Stored in directory: c:\\users\\017924631\\appdata\\local\\pip\\cache\\wheels\\0f\\f0\\3d\\517368b8ce80486e84f89f214e0a022554e4ee64969f46279b\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
     ]
    }
   ],
   "source": [
    "!python.exe -m pip install --upgrade pip\n",
    "!pip3 install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession # importing pyspark library\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "display(HTML('<style>pre { white-space: pre !important; }</style>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate() # instanciando uma SparkSession"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Importando dados**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_df(name):\n",
    "    import os\n",
    "\n",
    "    for root, dirs, files in os.walk(os.getcwd().replace('\\\\','/')):\n",
    "        if name in files:\n",
    "            return os.path.join(root, name)\n",
    "        else:\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dicionário da tabela:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_name</th>\n",
       "      <th>data_type</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ts_atl_tran</td>\n",
       "      <td>timestamp</td>\n",
       "      <td>Data e hora de atualizacao das informacoes de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cd_ct_crt</td>\n",
       "      <td>char(100)</td>\n",
       "      <td>Identifica de forma unica a conta pagamento po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cd_idfr_tran</td>\n",
       "      <td>char(100)</td>\n",
       "      <td>Codigo ou identificador unico prestado pela in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       col_name  data_type                                            comment\n",
       "0   ts_atl_tran  timestamp  Data e hora de atualizacao das informacoes de ...\n",
       "1     cd_ct_crt  char(100)  Identifica de forma unica a conta pagamento po...\n",
       "2  cd_idfr_tran  char(100)  Codigo ou identificador unico prestado pela in..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_path = find_df('dict_tran_crt.csv')\n",
    "df_dict = pandas.read_csv(df_path, sep=';')\n",
    "df_dict.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200 entries, 0 to 199\n",
      "Data columns (total 27 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   ts_atl_tran       200 non-null    object \n",
      " 1   cd_idfr_tran      200 non-null    object \n",
      " 2   tx_idfr_crt       199 non-null    float64\n",
      " 3   nm_lncd           194 non-null    object \n",
      " 4   nm_tran           199 non-null    object \n",
      " 5   cd_idfr_fat       195 non-null    object \n",
      " 6   tx_tip_lcto       199 non-null    object \n",
      " 7   tx_tip_tran       199 non-null    object \n",
      " 8   tx_adc_tran       199 non-null    object \n",
      " 9   tx_tip_pgto_tran  199 non-null    object \n",
      " 10  tx_tip_tarf       199 non-null    object \n",
      " 11  tx_adc_tip_tarf   199 non-null    object \n",
      " 12  tx_tip_opr_crd    199 non-null    object \n",
      " 13  tx_adc_opr_crd    199 non-null    object \n",
      " 14  tx_idfr_pcl       199 non-null    object \n",
      " 15  qt_pcl_tran       199 non-null    float64\n",
      " 16  vl_moen           199 non-null    float64\n",
      " 17  vl_extr           199 non-null    float64\n",
      " 18  cd_moe_vl_extr    199 non-null    object \n",
      " 19  dt_tran           199 non-null    object \n",
      " 20  dt_lcto_fat       199 non-null    object \n",
      " 21  cd_ctgr_cia       199 non-null    float64\n",
      " 22  cd_tip_pss        200 non-null    int64  \n",
      " 23  cd_tip_mvt_opb    200 non-null    object \n",
      " 24  nm_ifc_dst        200 non-null    object \n",
      " 25  nm_prmo_arq_ogm   200 non-null    object \n",
      " 26  dt_arq_ogm        200 non-null    object \n",
      "dtypes: float64(5), int64(1), object(21)\n",
      "memory usage: 42.3+ KB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_path = find_df('tran_anonimizada.csv')\n",
    "\n",
    "df_pandas = pandas.read_csv(df_path)\n",
    "\n",
    "df_pandas.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_nanfix(dataframe: pandas.DataFrame) -> pandas.DataFrame:\n",
    "    '''\n",
    "    Quando uma coluna no pandas dataframe contém\n",
    "    valores NaN. O PySpark tem problemas para conversão.\n",
    "    Portanto, solucionaremos este bug convertendo os valores\n",
    "    NaN em '', para fazermos a conversão.\n",
    "    '''\n",
    "    df_header = list(dataframe.columns)\n",
    "    columns_nan =   [ \n",
    "                        column for column in df_header \n",
    "                            if dataframe[column].isnull()\n",
    "                            .values.any() \n",
    "                    ]\n",
    "\n",
    "\n",
    "    for column in columns_nan:\n",
    "        dataframe[column] = dataframe[column].fillna('')\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "def column_change_datatype(dataframe: pandas.DataFrame, df_list: list) -> pandas.DataFrame:\n",
    "    df_header = list(dataframe.columns)\n",
    "    for column in df_header:\n",
    "        if column in df_list:\n",
    "            dataframe[column] = dataframe[column].astype(str)\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict_columns         = [ columns for columns in df_dict['col_name'] ]\n",
    "df_dict_dtypes          = [ dtypes for dtypes in df_dict['data_type'] ]\n",
    "df_dict_columns_dtypes  = dict(zip( df_dict_columns, df_dict_dtypes ))\n",
    "df_dict_columns_str     = [ chave for chave, valor in \n",
    "                          df_dict_columns_dtypes.items() if 'char' in valor ]\n",
    "\n",
    "df_pandas_strfix        = column_change_datatype(df_pandas, df_dict_columns_str)\n",
    "df_pandas_nanfix        = column_nanfix(df_pandas_strfix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\017924631\\Desktop\\working-tree\\ibm-school-of-data_engineering\\6-spark\\ambvir\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "c:\\Users\\017924631\\Desktop\\working-tree\\ibm-school-of-data_engineering\\6-spark\\ambvir\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "field qt_pcl_tran: Can not merge type <class 'pyspark.sql.types.DoubleType'> and <class 'pyspark.sql.types.StringType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_raw \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mcreateDataFrame(df_pandas)\n\u001b[0;32m      2\u001b[0m df_raw\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\017924631\\Desktop\\working-tree\\ibm-school-of-data_engineering\\6-spark\\ambvir\\lib\\site-packages\\pyspark\\sql\\session.py:891\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    888\u001b[0m     has_pandas \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \u001b[39mif\u001b[39;00m has_pandas \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(data, pandas\u001b[39m.\u001b[39mDataFrame):\n\u001b[0;32m    890\u001b[0m     \u001b[39m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m--> 891\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(SparkSession, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mcreateDataFrame(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m    892\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[0;32m    893\u001b[0m     )\n\u001b[0;32m    894\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_dataframe(\n\u001b[0;32m    895\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    896\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\017924631\\Desktop\\working-tree\\ibm-school-of-data_engineering\\6-spark\\ambvir\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:437\u001b[0m, in \u001b[0;36mSparkConversionMixin.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    435\u001b[0m             \u001b[39mraise\u001b[39;00m\n\u001b[0;32m    436\u001b[0m converted_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_from_pandas(data, schema, timezone)\n\u001b[1;32m--> 437\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_dataframe(converted_data, schema, samplingRatio, verifySchema)\n",
      "File \u001b[1;32mc:\\Users\\017924631\\Desktop\\working-tree\\ibm-school-of-data_engineering\\6-spark\\ambvir\\lib\\site-packages\\pyspark\\sql\\session.py:936\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    934\u001b[0m     rdd, struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_createFromRDD(data\u001b[39m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[0;32m    935\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 936\u001b[0m     rdd, struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_createFromLocal(\u001b[39mmap\u001b[39;49m(prepare, data), schema)\n\u001b[0;32m    937\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    938\u001b[0m jrdd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mSerDeUtil\u001b[39m.\u001b[39mtoJavaArray(rdd\u001b[39m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[1;32mc:\\Users\\017924631\\Desktop\\working-tree\\ibm-school-of-data_engineering\\6-spark\\ambvir\\lib\\site-packages\\pyspark\\sql\\session.py:631\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[1;34m(self, data, schema)\u001b[0m\n\u001b[0;32m    628\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(data)\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m schema \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(schema, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m--> 631\u001b[0m     struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inferSchemaFromList(data, names\u001b[39m=\u001b[39;49mschema)\n\u001b[0;32m    632\u001b[0m     converter \u001b[39m=\u001b[39m _create_converter(struct)\n\u001b[0;32m    633\u001b[0m     tupled_data: Iterable[Tuple] \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(converter, data)\n",
      "File \u001b[1;32mc:\\Users\\017924631\\Desktop\\working-tree\\ibm-school-of-data_engineering\\6-spark\\ambvir\\lib\\site-packages\\pyspark\\sql\\session.py:517\u001b[0m, in \u001b[0;36mSparkSession._inferSchemaFromList\u001b[1;34m(self, data, names)\u001b[0m\n\u001b[0;32m    515\u001b[0m infer_dict_as_struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jconf\u001b[39m.\u001b[39minferDictAsStruct()\n\u001b[0;32m    516\u001b[0m prefer_timestamp_ntz \u001b[39m=\u001b[39m is_timestamp_ntz_preferred()\n\u001b[1;32m--> 517\u001b[0m schema \u001b[39m=\u001b[39m reduce(\n\u001b[0;32m    518\u001b[0m     _merge_type,\n\u001b[0;32m    519\u001b[0m     (_infer_schema(row, names, infer_dict_as_struct, prefer_timestamp_ntz) \u001b[39mfor\u001b[39;49;00m row \u001b[39min\u001b[39;49;00m data),\n\u001b[0;32m    520\u001b[0m )\n\u001b[0;32m    521\u001b[0m \u001b[39mif\u001b[39;00m _has_nulltype(schema):\n\u001b[0;32m    522\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mSome of types cannot be determined after inferring\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\017924631\\Desktop\\working-tree\\ibm-school-of-data_engineering\\6-spark\\ambvir\\lib\\site-packages\\pyspark\\sql\\types.py:1383\u001b[0m, in \u001b[0;36m_merge_type\u001b[1;34m(a, b, name)\u001b[0m\n\u001b[0;32m   1381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(a, StructType):\n\u001b[0;32m   1382\u001b[0m     nfs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m((f\u001b[39m.\u001b[39mname, f\u001b[39m.\u001b[39mdataType) \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m cast(StructType, b)\u001b[39m.\u001b[39mfields)\n\u001b[1;32m-> 1383\u001b[0m     fields \u001b[39m=\u001b[39m [\n\u001b[0;32m   1384\u001b[0m         StructField(\n\u001b[0;32m   1385\u001b[0m             f\u001b[39m.\u001b[39mname, _merge_type(f\u001b[39m.\u001b[39mdataType, nfs\u001b[39m.\u001b[39mget(f\u001b[39m.\u001b[39mname, NullType()), name\u001b[39m=\u001b[39mnew_name(f\u001b[39m.\u001b[39mname))\n\u001b[0;32m   1386\u001b[0m         )\n\u001b[0;32m   1387\u001b[0m         \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m a\u001b[39m.\u001b[39mfields\n\u001b[0;32m   1388\u001b[0m     ]\n\u001b[0;32m   1389\u001b[0m     names \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m([f\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m fields])\n\u001b[0;32m   1390\u001b[0m     \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m nfs:\n",
      "File \u001b[1;32mc:\\Users\\017924631\\Desktop\\working-tree\\ibm-school-of-data_engineering\\6-spark\\ambvir\\lib\\site-packages\\pyspark\\sql\\types.py:1385\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(a, StructType):\n\u001b[0;32m   1382\u001b[0m     nfs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m((f\u001b[39m.\u001b[39mname, f\u001b[39m.\u001b[39mdataType) \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m cast(StructType, b)\u001b[39m.\u001b[39mfields)\n\u001b[0;32m   1383\u001b[0m     fields \u001b[39m=\u001b[39m [\n\u001b[0;32m   1384\u001b[0m         StructField(\n\u001b[1;32m-> 1385\u001b[0m             f\u001b[39m.\u001b[39mname, _merge_type(f\u001b[39m.\u001b[39;49mdataType, nfs\u001b[39m.\u001b[39;49mget(f\u001b[39m.\u001b[39;49mname, NullType()), name\u001b[39m=\u001b[39;49mnew_name(f\u001b[39m.\u001b[39;49mname))\n\u001b[0;32m   1386\u001b[0m         )\n\u001b[0;32m   1387\u001b[0m         \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m a\u001b[39m.\u001b[39mfields\n\u001b[0;32m   1388\u001b[0m     ]\n\u001b[0;32m   1389\u001b[0m     names \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m([f\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m fields])\n\u001b[0;32m   1390\u001b[0m     \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m nfs:\n",
      "File \u001b[1;32mc:\\Users\\017924631\\Desktop\\working-tree\\ibm-school-of-data_engineering\\6-spark\\ambvir\\lib\\site-packages\\pyspark\\sql\\types.py:1378\u001b[0m, in \u001b[0;36m_merge_type\u001b[1;34m(a, b, name)\u001b[0m\n\u001b[0;32m   1375\u001b[0m     \u001b[39mreturn\u001b[39;00m b\n\u001b[0;32m   1376\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(a) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mtype\u001b[39m(b):\n\u001b[0;32m   1377\u001b[0m     \u001b[39m# TODO: type cast (such as int -> long)\u001b[39;00m\n\u001b[1;32m-> 1378\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(new_msg(\u001b[39m\"\u001b[39m\u001b[39mCan not merge type \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mtype\u001b[39m(a), \u001b[39mtype\u001b[39m(b))))\n\u001b[0;32m   1380\u001b[0m \u001b[39m# same type\u001b[39;00m\n\u001b[0;32m   1381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(a, StructType):\n",
      "\u001b[1;31mTypeError\u001b[0m: field qt_pcl_tran: Can not merge type <class 'pyspark.sql.types.DoubleType'> and <class 'pyspark.sql.types.StringType'>"
     ]
    }
   ],
   "source": [
    "df_raw = spark.createDataFrame(df_pandas)\n",
    "df_raw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-----------+-----------------+--------------------+--------------------+-----------+--------------------+--------------------+----------------+-----------+--------------------+-------------------+--------------------+------------------+-----------+-------+-------+--------------+----------+-----------+-----------+----------+--------------+-------------------+--------------------+----------+\n",
      "|        ts_atl_tran|        cd_idfr_tran|tx_idfr_crt|          nm_lncd|             nm_tran|         cd_idfr_fat|tx_tip_lcto|         tx_tip_tran|         tx_adc_tran|tx_tip_pgto_tran|tx_tip_tarf|     tx_adc_tip_tarf|     tx_tip_opr_crd|      tx_adc_opr_crd|       tx_idfr_pcl|qt_pcl_tran|vl_moen|vl_extr|cd_moe_vl_extr|   dt_tran|dt_lcto_fat|cd_ctgr_cia|cd_tip_pss|cd_tip_mvt_opb|         nm_ifc_dst|     nm_prmo_arq_ogm|dt_arq_ogm|\n",
      "+-------------------+--------------------+-----------+-----------------+--------------------+--------------------+-----------+--------------------+--------------------+----------------+-----------+--------------------+-------------------+--------------------+------------------+-----------+-------+-------+--------------+----------+-----------+-----------+----------+--------------+-------------------+--------------------+----------+\n",
      "|2022-06-06 01:05:49|e88dcaa44f282222c...|       5218|CREDITO_PARCELADO|     GIRO AUTO PECAS|                 010|     DEBITO|           PAGAMENTO|                    |         A_PRAZO|      OUTRA|transactionType_n...|             OUTROS|transactionType_n...|       PARCELA_001|          3|  54.37|  54.37|           BRL|2021-08-18| 2021-08-19|          8|         1|             R|BANCO DO BRASIL S/A|kfka.opb.r.get.ca...| 20220606R|\n",
      "|2022-06-06 03:11:25|06fdf69d-aff4-4af...|       6895|  CREDITO_A_VISTA|         ENZOBRASILF|            19395104|     DEBITO|           PAGAMENTO|                    |         A_VISTA|      OUTRA|  Fee type: purchase|             OUTROS|                    |                 1|          1| 125.00|   0.00|           BRL|2022-04-01| 2022-04-01|       5046|         1|             R|BANCO DO BRASIL S/A|kfka.opb.r.get.ca...| 20220606R|\n",
      "|2022-06-06 01:45:26|0d2222da-e82e-44d...|       1853|           OUTROS|     EBANX EPICGAMES|            99999999|     DEBITO|           PAGAMENTO|                  NA|         A_VISTA|      OUTRA|                  NA|             OUTROS|                  NA|                 0|          0|  12.45|   0.00|           BRL|2022-05-24| 2022-05-25|       6513|         1|             R|BANCO DO BRASIL S/A|kfka.opb.r.get.ca...| 20220606R|\n",
      "|2022-06-06 03:23:21|dac80468-4969-4d8...|       2796|  CREDITO_A_VISTA| SUPERMERCADOSOLUCAO|            23574311|     DEBITO|           PAGAMENTO|                    |         A_VISTA|      OUTRA|  Fee type: purchase|             OUTROS|                    |                 1|          1|  40.15|   0.00|           BRL|2022-04-30| 2022-04-30|       5411|         1|             R|BANCO DO BRASIL S/A|kfka.opb.r.get.ca...| 20220606R|\n",
      "|2022-06-06 04:06:42|0GP8H6GBxAM8znX44...|       4089|  CREDITO_A_VISTA|        POSTOBARAUNA|            11382028|     DEBITO|           PAGAMENTO|                    |         A_VISTA|      OUTRA|  Fee type: purchase|             OUTROS|                    |                 1|          1|  20.00|   0.00|           BRL|2022-01-11| 2022-01-11|       5541|         1|             R|BANCO DO BRASIL S/A|kfka.opb.r.get.ca...| 20220606R|\n",
      "|2022-06-06 15:39:51|4f4f24c8-b26c-46b...|       6371|  CREDITO_A_VISTA|  99APP       *99App|              202206|     DEBITO|              OUTROS|COMPRA A VISTA MA...|         A_VISTA|      OUTRA|COMPRA A VISTA MA...|             OUTROS|COMPRA A VISTA MA...|99APP       *99App|          1|   9.90|   9.90|           BRL|2022-05-31| 2022-06-01|       4121|         1|             R|BANCO DO BRASIL S/A|kfka.opb.r.get.ca...| 20220606R|\n",
      "|2022-06-06 01:52:11|f4bc829226f288f4c...|       NULL|             NULL|                NULL|                NULL|       NULL|                NULL|                NULL|            NULL|       NULL|                NULL|               NULL|                NULL|              NULL|       NULL|   NULL|   NULL|          NULL|      NULL|       NULL|       NULL|         1|             R|BANCO DO BRASIL S/A|kfka.opb.r.get.ca...| 20220606R|\n",
      "|2022-06-06 01:55:41|642646c8-adbb-48d...|       0250|  CREDITO_A_VISTA|   MERCPAGTOPOECORTE|            22067890|     DEBITO|           PAGAMENTO|                    |         A_VISTA|      OUTRA|  Fee type: purchase|             OUTROS|                    |                 1|          1|   2.50|   0.00|           BRL|2022-04-18| 2022-04-18|       5947|         1|             R|BANCO DO BRASIL S/A|kfka.opb.r.get.ca...| 20220606R|\n",
      "|2022-06-06 02:11:38|ca96fcfe-48b4-4f9...|       0150|  CREDITO_A_VISTA|PAGSAMUELGOMESDACRUZ|            21207598|     DEBITO|           PAGAMENTO|                    |         A_PRAZO|      OUTRA|  Fee type: purchase|             OUTROS|                    |                 4|         12| 281.12|   0.00|           BRL|2022-01-14| 2022-01-14|       5311|         1|             R|BANCO DO BRASIL S/A|kfka.opb.r.get.ca...| 20220606R|\n",
      "|2022-06-06 14:15:05|b408e4a2-d6eb-49e...|       7994|           OUTROS|PARCELA DE REFINA...|14bc144f-09cd-4ce...|     DEBITO|OPERACOES_CREDITO...|                  NA|         A_PRAZO|      OUTRA|                  NA|PARCELAMENTO_FATURA|                  NA|                 2|          2| 297.72|   0.00|           BRL|2021-12-07| 2021-12-07|       5651|         1|             R|BANCO DO BRASIL S/A|kfka.opb.r.get.ca...| 20220606R|\n",
      "|2022-06-06 01:15:07|968244a2-2622-49a...|       1826|           OUTROS|        DESCART FLEX|            99999999|     DEBITO|           PAGAMENTO|                  NA|         A_VISTA|      OUTRA|                  NA|             OUTROS|                  NA|                 0|          0|  63.90|   0.00|           BRL|2022-06-03| 2022-06-04|       5411|         1|             R|BANCO DO BRASIL S/A|kfka.opb.r.get.ca...| 20220606R|\n",
      "|2022-06-06 01:14:40|90026cf6-8e6b-4fb...|       9439|           OUTROS|SEG CARTAO PROTEGIDO|            20220608|     DEBITO|              TARIFA|                  NA|         A_VISTA|      OUTRA|              SEGURO|             OUTROS|                  NA|                 0|          0|   7.90|   0.00|           BRL|2022-05-18| 2022-05-19|       6834|         1|             R|BANCO DO BRASIL S/A|kfka.opb.r.get.ca...| 20220606R|\n",
      "|2022-06-06 00:46:45|60a642f0-cf84-424...|       1001|           OUTROS|  ifood       *ifood|            20220606|     DEBITO|           PAGAMENTO|                  NA|         A_VISTA|      OUTRA|                  NA|             OUTROS|                  NA|                 0|          0|  93.99|   0.00|           BRL|2022-05-20| 2022-05-21|       5814|         1|             R|BANCO DO BRASIL S/A|kfka.opb.r.get.ca...| 20220606R|\n",
      "|2022-06-06 17:54:56|04df260f-d862-46c...|       9497|           OUTROS|D PEREIRA COMERCI...|            20220609|     DEBITO|           PAGAMENTO|                  NA|         A_VISTA|      OUTRA|                  NA|             OUTROS|                  NA|                 0|          0|  51.00|   0.00|           BRL|2022-05-14| 2022-05-15|       5999|         1|             R|BANCO DO BRASIL S/A|kfka.opb.r.get.ca...| 20220606R|\n",
      "|2022-06-06 02:59:00|f422066e-9084-4ec...|       8757|  CREDITO_A_VISTA|   MIXMATEUSPINHEIRO|            16230574|     DEBITO|           PAGAMENTO|                    |         A_PRAZO|      OUTRA|  Fee type: purchase|             OUTROS|                    |                 1|          5|  70.24|   0.00|           BRL|2022-03-04| 2022-03-04|       5411|         1|             R|BANCO DO BRASIL S/A|kfka.opb.r.get.ca...| 20220606R|\n",
      "|2022-06-06 01:16:43|646b664880a28e08d...|       5596|  CREDITO_A_VISTA|         HUDSON NEWS|                 152|     DEBITO|           PAGAMENTO|                    |         A_VISTA|      OUTRA|transactionType_n...|             OUTROS|transactionType_n...|       SEM_PARCELA|          0|  42.00|  42.00|           BRL|2022-04-24| 2022-04-25|         15|         1|             R|BANCO DO BRASIL S/A|kfka.opb.r.get.ca...| 20220606R|\n",
      "|2022-06-06 00:58:39|    4982000222889266|       3566|           OUTROS|  ifood       *ifood| 4981000222889166001|     DEBITO|           PAGAMENTO|     COMPRA NACIONAL|         A_VISTA|      OUTRA|                  NA|             OUTROS|                  NA|                NA|          0|  30.97|   0.00|           BRL|2022-04-28| 2022-04-29|       5814|         1|             R|BANCO DO BRASIL S/A|kfka.opb.r.get.ca...| 20220606R|\n",
      "|2022-06-06 08:11:40|4Pg4SCp6Dc4wKCU84...|       4609|  CREDITO_A_VISTA|          FSSCOSTAME|            19793764|     DEBITO|           PAGAMENTO|                    |         A_VISTA|      OUTRA|  Fee type: purchase|             OUTROS|                    |                 1|          1|  46.00|   0.00|           BRL|2022-04-17| 2022-04-17|       5499|         1|             R|BANCO DO BRASIL S/A|kfka.opb.r.get.ca...| 20220606R|\n",
      "|2022-06-06 00:44:34|28648684f662e2069...|       5394|  CREDITO_A_VISTA|      APPLE.COM/BILL|                  13|     DEBITO|           PAGAMENTO|                    |         A_VISTA|      OUTRA|transactionType_n...|             OUTROS|transactionType_n...|       SEM_PARCELA|          0|  25.90|  25.90|           BRL|2021-09-17| 2021-09-18|         15|         1|             R|BANCO DO BRASIL S/A|kfka.opb.r.get.ca...| 20220606R|\n",
      "|2022-06-06 04:10:44|p99d6TpD2zlZ6pq9l...|       4961|  CREDITO_A_VISTA|IFDECONOMICOSUPER...|             2996154|     DEBITO|           PAGAMENTO|                    |         A_VISTA|      OUTRA|  Fee type: purchase|             OUTROS|                    |                 1|          1|  74.44|   0.00|           BRL|2021-10-04| 2021-10-04|       5411|         1|             R|BANCO DO BRASIL S/A|kfka.opb.r.get.ca...| 20220606R|\n",
      "+-------------------+--------------------+-----------+-----------------+--------------------+--------------------+-----------+--------------------+--------------------+----------------+-----------+--------------------+-------------------+--------------------+------------------+-----------+-------+-------+--------------+----------+-----------+-----------+----------+--------------+-------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "\n",
    "df_raw =  spark.read.csv (\n",
    "            find_df('tran_anonimizada.csv'), \n",
    "            header=True, \n",
    "            sep=',', \n",
    "            inferSchema=True, \n",
    "            quote='\"', \n",
    "            escape='\"'\n",
    ")\n",
    "\n",
    "for col in df_raw.columns:\n",
    "    df_raw = df_raw.withColumn(col, f.trim(f.col(col)))\n",
    "\n",
    "df_raw.show(vertical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ts_atl_tran: timestamp, cd_idfr_tran: string, tx_idfr_crt: string, nm_lncd: string, nm_tran: string, cd_idfr_fat: string, tx_tip_lcto: string, tx_tip_tran: string, tx_adc_tran: string, tx_tip_pgto_tran: string, tx_tip_tarf: string, tx_adc_tip_tarf: string, tx_tip_opr_crd: string, tx_adc_opr_crd: string, tx_idfr_pcl: string, qt_pcl_tran: string, vl_moen: string, vl_extr: string, cd_moe_vl_extr: string, dt_tran: string, dt_lcto_fat: string, cd_ctgr_cia: string, cd_tip_pss: int, cd_tip_mvt_opb: string, nm_ifc_dst: string, nm_prmo_arq_ogm: string, dt_arq_ogm: string]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ambvir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "76beb9e015ca53069c8a997ceb5a51e2d81ca4e14a5906d0d4caf8e292f4e1ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
